{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9gUQpaBxNa"
      },
      "source": [
        "# How to Train YOLOv7 on a Custom Dataset\n",
        "\n",
        "This tutorial is based on the [YOLOv7 repository](https://github.com/WongKinYiu/yolov7) by WongKinYiu. This notebook shows training on **your own custom objects**. Many thanks to WongKinYiu and AlexeyAB for putting this repository together.\n",
        "\n",
        "\n",
        "### **Accompanying Blog Post**\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on [how to train YOLOv7](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/), concurrently.\n",
        "\n",
        "### **Steps Covered in this Tutorial**\n",
        "\n",
        "To train our detector we take the following steps:\n",
        "\n",
        "* Install YOLOv7 dependencies\n",
        "* Load custom dataset from Roboflow in YOLOv7 format\n",
        "* Run YOLOv7 training\n",
        "* Evaluate YOLOv7 performance\n",
        "* Run YOLOv7 inference on test images\n",
        "* OPTIONAL: Deployment\n",
        "* OPTIONAL: Active Learning\n",
        "\n",
        "\n",
        "### Preparing a Custom Dataset\n",
        "\n",
        "In this tutorial, we will utilize an open source computer vision dataset from one of the 90,000+ available on [Roboflow Universe](https://universe.roboflow.com).\n",
        "\n",
        "If you already have your own images (and, optionally, annotations), you can convert your dataset using [Roboflow](https://roboflow.com), a set of tools developers use to build better computer vision models quickly and accurately. 100k+ developers use roboflow for (automatic) annotation, converting dataset formats (like to YOLOv7), training, deploying, and improving their datasets/models.\n",
        "\n",
        "Follow [the getting started guide here](https://docs.roboflow.com/quick-start) to create and prepare your own custom dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Install Dependencies\n",
        "\n",
        "_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGJspIl90XhV",
        "outputId": "367ccff8-b24e-4e1a-9ead-5620f38b8f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Oct 23 10:02:23 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650        Off | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   50C    P8               2W /  30W |     54MiB /  4096MiB |     17%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      2736      G   /usr/lib/xorg/Xorg                           53MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD-uPyQ_2jiN",
        "outputId": "4daaf7c1-1505-4781-bfab-37679c1a384b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 579, done.\u001b[K\n",
            "remote: Total 579 (delta 0), reused 0 (delta 0), pack-reused 579\u001b[K\n",
            "Receiving objects: 100% (579/579), 38.53 MiB | 5.36 MiB/s, done.\n",
            "Resolving deltas: 100% (280/280), done.\n",
            "/home/mercedes/.local/share/Trash/files/yolov7.6/yolov7/yolov7\n",
            "Branch 'fix/problems_associated_with_the_latest_versions_of_pytorch_and_numpy' set up to track remote branch 'fix/problems_associated_with_the_latest_versions_of_pytorch_and_numpy' from 'origin'.\n",
            "Switched to a new branch 'fix/problems_associated_with_the_latest_versions_of_pytorch_and_numpy'\n"
          ]
        }
      ],
      "source": [
        "# Download YOLOv7 repository and install requirements\n",
        "\n",
        "# !git clone https://github.com/WongKinYiu/yolov7\n",
        "# %cd yolov7\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# current version of YOLOv7 is not compatible with pytorch>1.12.1 and numpy>1.20.1\n",
        "# until the appropriate changes get made to the main repository, we will be using a fork containing the patched code\n",
        "# you can track the progress here: https://github.com/roboflow/notebooks/issues/27\n",
        "\n",
        "# !git clone https://github.com/SkalskiP/yolov7.git\n",
        "# %cd yolov7\n",
        "# !git checkout fix/problems_associated_with_the_latest_versions_of_pytorch_and_numpy\n",
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtJ24mPlyF-S"
      },
      "source": [
        "# Download Correctly Formatted Custom Data\n",
        "\n",
        "Next, we'll download our dataset in the right format. Use the `YOLOv7 PyTorch` export. Note that this model requires YOLO TXT annotations, a custom YAML file, and organized directories. The roboflow export writes this for us and saves it in the correct spot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovKgrVN8ygdW",
        "outputId": "b04c6b11-53d2-4f72-f5c9-efb096757625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in cat_dog_crawling-2 to yolov7pytorch:: 100%|██████████| 17022/17022 [00:01<00:00, 10229.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to cat_dog_crawling-2 in yolov7pytorch:: 100%|██████████| 1535/1535 [00:00<00:00, 7339.75it/s]\n"
          ]
        }
      ],
      "source": [
        "# REPLACE with your custom code snippet generated above\n",
        "\n",
        "#!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"7kIgVpnvd8c5pAl1n1ad\")\n",
        "# project = rf.workspace(\"cedulas-real\").project(\"real-cedulas\")\n",
        "# dataset = project.version(7).download(\"yolov7\")\n",
        "\n",
        "# project = rf.workspace(\"brad-dwyer\").project(\"oxford-pets\")\n",
        "# dataset = project.version(3).download(\"yolov7\")\n",
        "\n",
        "# project = rf.workspace(\"roboflow-58fyf\").project(\"rock-paper-scissors-sxsw\")\n",
        "# dataset = project.version(12).download(\"yolov7\")\n",
        "\n",
        "# project = rf.workspace(\"yolones\").project(\"animal_detection-xqcka\")\n",
        "# dataset = project.version(1).download(\"yolov7\")\n",
        "\n",
        "# project = rf.workspace(\"yolo-ja0zo\").project(\"dogs-or-cats-faipe\")\n",
        "# dataset = project.version(1).download(\"yolov7\")\n",
        "\n",
        "project = rf.workspace(\"tbvjzsk-naver-com\").project(\"cat_dog_crawling\")\n",
        "dataset = project.version(2).download(\"yolov7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHfT9gEiBsBd"
      },
      "source": [
        "# Begin Custom Training\n",
        "\n",
        "We're ready to start custom training.\n",
        "\n",
        "NOTE: We will only modify one of the YOLOv7 training defaults in our example: `epochs`. We will adjust from 300 to 100 epochs in our example for speed. If you'd like to change other settings, see details in [our accompanying blog post](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUbmy674bhpD",
        "outputId": "181fd7c4-5d09-47b2-c9d0-3c28fa57886d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/yolov7'\n",
            "/home/mercedes/Documentos/GRADUACAO-UFPA/07Bloco[2023.2-2023.4]/Inteligencia_Computacional[Aldebaro]/Atividades/Trab.03/model/yolov7/yolov7\n",
            "--2023-10-22 09:35:00--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\n",
            "Resolvendo github.com (github.com)... 20.201.28.151\n",
            "Conectando-se a github.com (github.com)|20.201.28.151|:443... conectado.\n",
            "A requisição HTTP foi enviada, aguardando resposta... 302 Found\n",
            "Localização: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/13e046d1-f7f0-43ab-910b-480613181b1f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231022T123500Z&X-Amz-Expires=300&X-Amz-Signature=c5f677981f9c54bcf26ae0b225f97838000eb8c8aec41e905b3af619d8cd0548&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7_training.pt&response-content-type=application%2Foctet-stream [redirecionando]\n",
            "--2023-10-22 09:35:01--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/13e046d1-f7f0-43ab-910b-480613181b1f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231022T123500Z&X-Amz-Expires=300&X-Amz-Signature=c5f677981f9c54bcf26ae0b225f97838000eb8c8aec41e905b3af619d8cd0548&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7_training.pt&response-content-type=application%2Foctet-stream\n",
            "Resolvendo objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Conectando-se a objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... conectado.\n",
            "A requisição HTTP foi enviada, aguardando resposta... 200 OK\n",
            "Tamanho: 75628875 (72M) [application/octet-stream]\n",
            "Salvando em: ‘yolov7_training.pt.1’\n",
            "\n",
            "yolov7_training.pt. 100%[===================>]  72,12M  7,41MB/s    em 10s     \n",
            "\n",
            "2023-10-22 09:35:11 (7,01 MB/s) - ‘yolov7_training.pt.1’ salvo [75628875/75628875]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # download COCO starting checkpoint\n",
        "# %cd /content/yolov7\n",
        "# !wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iqOPKjr22mL",
        "outputId": "7de100db-3fa3-4d1e-acba-636b2d0736f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/yolov7'\n",
            "/home/mercedes/Documentos/GRADUACAO-UFPA/07Bloco[2023.2-2023.4]/Inteligencia_Computacional[Aldebaro]/Atividades/Trab.03/model/yolov7\n"
          ]
        }
      ],
      "source": [
        "# run this cell to begin training\n",
        "%cd /content/yolov7\n",
        "# !python train.py --batch 7 --epochs 2 --data {dataset.location}/data.yaml --weights 'yolov7_training.pt' --device 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W0MpUaTCJro"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "We can evaluate the performance of our custom training using the provided evalution script.\n",
        "\n",
        "Note we can adjust the below custom arguments. For details, see [the arguments accepted by detect.py](https://github.com/WongKinYiu/yolov7/blob/main/detect.py#L154)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N4cfnLtTCIce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(weights=['runs/train/exp3/weights/best.pt'], source='/home/mercedes/Documentos/GRADUACAO-UFPA/07Bloco[2023.2-2023.4]/Inteligencia_Computacional[Aldebaro]/Atividades/Trab.03/model/yolov7/dogs-or-cats-1/test/images', img_size=640, conf_thres=0.3, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR 🚀 b2a7de9 torch 2.1.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1650, 3903.875MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/home/mercedes/Documentos/GRADUACAO-UFPA/07Bloco[2023.2-2023.4]/Inteligencia_Computacional[Aldebaro]/Atividades/Trab.03/model/train-yolov7/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36487166 parameters, 6194944 gradients, 103.2 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mercedes/Documentos/GRADUACAO-UFPA/07Bloco[2023.2-2023.4]/Inteligencia_Computacional[Aldebaro]/Atividades/Trab.03/model/yolov7/detect.py\", line 196, in <module>\n",
            "    detect()\n",
            "  File \"/home/mercedes/Documentos/GRADUACAO-UFPA/07Bloco[2023.2-2023.4]/Inteligencia_Computacional[Aldebaro]/Atividades/Trab.03/model/yolov7/detect.py\", line 57, in detect\n",
            "    dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mercedes/Documentos/GRADUACAO-UFPA/07Bloco[2023.2-2023.4]/Inteligencia_Computacional[Aldebaro]/Atividades/Trab.03/model/yolov7/utils/datasets.py\", line 154, in __init__\n",
            "    assert self.nf > 0, f'No images or videos found in {p}. ' \\\n",
            "           ^^^^^^^^^^^\n",
            "AssertionError: No images or videos found in /home/mercedes/Documentos/GRADUACAO-UFPA/07Bloco[2023.2-2023.4]/Inteligencia_Computacional[Aldebaro]/Atividades/Trab.03/model/yolov7/dogs-or-cats-1/test/images. Supported formats are:\n",
            "images: ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']\n",
            "videos: ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "!python detect.py --weights runs/train/exp3/weights/best.pt --conf 0.3 --source {dataset.location}/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6AGhNOSSHY4_"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "i = 0\n",
        "limit = 5 # max images to print\n",
        "for imageName in glob.glob('/content/yolov7/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    if i < limit:\n",
        "      display(Image(filename=imageName))\n",
        "      print(\"\\n\")\n",
        "    i = i + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMumI7a2JDAN"
      },
      "source": [
        "# Reparameterize for Inference\n",
        "\n",
        "https://github.com/WongKinYiu/yolov7/blob/main/tools/reparameterization.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jn4kCtgKiGO"
      },
      "source": [
        "# OPTIONAL: Deployment\n",
        "\n",
        "To deploy, you'll need to export your weights and save them to use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWOok8abrCsL"
      },
      "outputs": [],
      "source": [
        "# optional, zip to download weights and results locally\n",
        "\n",
        "!zip -r export.zip runs/detect\n",
        "!zip -r export.zip runs/train/exp/weights/best.pt\n",
        "!zip export.zip runs/train/exp/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f41PvE5gKhYw"
      },
      "source": [
        "# OPTIONAL: Active Learning Example\n",
        "\n",
        "Once our first training run is complete, we should use our model to help identify which images are most problematic in order to investigate, annotate, and improve our dataset (and, therefore, model).\n",
        "\n",
        "To do that, we can execute code that automatically uploads images back to our hosted dataset if the image is a specific class or below a given confidence threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcINqQS7Kt3-"
      },
      "outputs": [],
      "source": [
        "# # setup access to your workspace\n",
        "# rf = Roboflow(api_key=\"YOUR_API_KEY\")                               # used above to load data\n",
        "# inference_project =  rf.workspace().project(\"YOUR_PROJECT_NAME\")    # used above to load data\n",
        "# model = inference_project.version(1).model\n",
        "\n",
        "# upload_project = rf.workspace().project(\"YOUR_PROJECT_NAME\")\n",
        "\n",
        "# print(\"inference reference point: \", inference_project)\n",
        "# print(\"upload destination: \", upload_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEl1NVE3LSD_"
      },
      "outputs": [],
      "source": [
        "# # example upload: if prediction is below a given confidence threshold, upload it\n",
        "\n",
        "# confidence_interval = [10,70]                                   # [lower_bound_percent, upper_bound_percent]\n",
        "\n",
        "# for prediction in predictions:                                  # predictions list to loop through\n",
        "#   if(prediction['confidence'] * 100 >= confidence_interval[0] and\n",
        "#           prediction['confidence'] * 100 <= confidence_interval[1]):\n",
        "\n",
        "#           # upload on success!\n",
        "#           print(' >> image uploaded!')\n",
        "#           upload_project.upload(image, num_retry_uploads=3)     # upload image in question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVpCFeU-K4gb"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Congratulations, you've trained a custom YOLOv7 model! Next, start thinking about deploying and [building an MLOps pipeline](https://docs.roboflow.com) so your model gets better the more data it sees in the wild."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
